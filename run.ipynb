{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 Machine Learning\n",
    "## Road Segmentation Project.\n",
    "\n",
    "This notebook has to  be runned in a Google Colab envioroment\n",
    "\n",
    "MIN group. \n",
    "\n",
    "Mathilde Chaffard - Irene Vardabasso - Niccol`o Venturini Degli Esposti -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the requirements.txt file on Google Colab and after run the line below to install the necessary requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import ImageFile, ImageEnhance, ImageFilter, Image\n",
    "import sys, getopt\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn import linear_model\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect google colab to google drive to acces to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive') #mount google drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the parameters and the submission directory. Here the submission will be create in a google drive folder called submission, inside the EPFL folder, so on Google Drive has to be present a folder called EPFL, inside of this foldere has to be present a folder called submission and also has to be import the data, inside of EPFL folder, in a folder called data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./drive/MyDrive/EPFL/data\"\n",
    "SUBMISSION_DIR = \"./drive/MyDrive/EPFL/submission/\"\n",
    "SUBMISSION_PATH = \"./drive/MyDrive/EPFL/submission/submission.csv\"\n",
    "PATCH_SIZE = 400 #value find with cross validation 400\n",
    "BATCH_SIZE = 20\n",
    "LR =  0.001 #value find with cross validation 0.001\n",
    "MAX_ITER = 50\n",
    "THRESHOLD = 0.25 #value find with cross validation 0.25\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TEST_SIZE = 0.2\n",
    "K_FOLD = 5\n",
    "SEED= 56\n",
    "THRESHOLD_VALIDATION_VECTOR = [0.15,0.17,0.20, 0.25, 0.30, 0.35] # best found 0.25\n",
    "LEARNING_RATE_VALIDATION_VECTOR = [0.0003, 0.0001, 0.001, 0.01]  # best found 0.001\n",
    "PATCH_SIZE_VALIDATION_VECTOR = [80,400]  # best found 400\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the pre-trained neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import torchvision.models as models\n",
    "\n",
    "# import deeplabv3Plus\n",
    "def get_deeplabplus(encoder_name = 'resnet34'):\n",
    "    net = smp.DeepLabV3Plus(\n",
    "    encoder_name=encoder_name,\n",
    "    encoder_depth = 5,\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                     # model output channels (number of classes in your dataset)\n",
    "    )\n",
    "\n",
    "    return net\n",
    "\n",
    "# import deeplabv3\n",
    "def deeplab_model(encoder_name = 'resnet34'):\n",
    "    net = smp.DeepLabV3(\n",
    "        encoder_name=encoder_name,\n",
    "        encoder_depth=5,\n",
    "        encoder_weights=\"imagenet\",  # use `imagenet` pre-trained weights for encoder initialization\n",
    "        in_channels=3,  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "        classes=1,  # model output channels (number of classes in your dataset)\n",
    "    )\n",
    "\n",
    "    return net\n",
    "\n",
    "# import Deeplabv3Plus with resnet50\n",
    "def get_resnet50(classes=1):\n",
    "    #Create a DeepLabV3 model with ResNet50 backbone\n",
    "    net = smp.DeepLabV3(\n",
    "        encoder_name='resnet50',  # Use ResNet50 as the encoder\n",
    "        encoder_depth=5,\n",
    "        encoder_weights=\"imagenet\",  # Use pre-trained weights for encoder initialization\n",
    "        in_channels=3,  # Model input channels (3 for RGB images)\n",
    "        classes=classes,  # Model output channels (1 for binary segmentation)\n",
    "    )\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helpers function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform an image into a tensor, since when importing we are sure that we get np.arrays\n",
    "def image_into_tensor(img:np.ndarray, device = None, divide = False):\n",
    "    img = np.transpose(img,[2,0,1]) #change the order of the dimensions\n",
    "    tensor = torch.Tensor(img) #transform the image into a tensor\n",
    "    if divide:\n",
    "        tensor = tensor / 255 #divide by 255 to have values between 0 and 1\n",
    "    tensor.unsqueeze(0)\n",
    "    if device is not None:\n",
    "        tensor = tensor.to(device) #send the tensor to the device\n",
    "    return tensor\n",
    "\n",
    "#transform a mask into a tensor, since when importing we are sure that we get np.arrays\n",
    "def mask_to_tensor(mask: np.ndarray, device = None):\n",
    "    tensor = transforms.ToTensor()(mask) #transform the image into a tensor\n",
    "    tensor = torch.round(tensor) #round the values to have only 0 and 1\n",
    "    if device is not None:\n",
    "        tensor = tensor.to(device) #send the tensor to the device\n",
    "    return tensor[0,:,:][None,:,:]\n",
    "\n",
    "#transform a tensor into a mask, since when importing we are sure that we get np.arrays\n",
    "def transform_to_patch(pixels, threshold):\n",
    "    m = np.mean(pixels) #compute the mean of the pixels\n",
    "    if m  > threshold:\n",
    "        return 1 #if the mean is above the threshold, we return 1\n",
    "    else:\n",
    "        return 0 #if the mean is below the threshold, we return 0\n",
    "\n",
    "#transform an image into a np array, since when importing we are sure that we get np.arrays\n",
    "def images_to_np_array(images):\n",
    "    return np.array([np.array(img) for img in images]) #transform the images into np.arrays\n",
    "\n",
    "#function to save a prediction changing the threshold to create the image for the report\n",
    "def report(prediction, th):\n",
    "    new_prediction = np.zeros(prediction.shape)\n",
    "    new_prediction[prediction > th] = 150 #if the prediction is above the threshold, we put 150, so it is visible in the image\n",
    "    return new_prediction\n",
    "\n",
    "#take a predected image and translate it into a prediction on patches\n",
    "def transform_prediction_to_patch(img, id, patch_size=16, step=16, th=0.25):\n",
    "    prs = []\n",
    "    ids = []\n",
    "    for j in range(0,img.shape[1],step):\n",
    "        for i in range(0, img.shape[0],step):\n",
    "            threshold = th \n",
    "            prs.append(transform_to_patch(img[i:i+patch_size,j:j+patch_size],threshold)) #transform the image into a patch\n",
    "            ids.append(\"{:03d}_{}_{}\".format(id, j, i)) #create the id of the patch\n",
    "    return prs, ids\n",
    "\n",
    "#takes a np.array from images, and transform into PIL image (which is more usable to deal with chages on the image)\n",
    "def PIL_Images_from_np_array(images):\n",
    "    return list(map(Image.fromarray, images)) #transform the np.arrays into PIL images\n",
    "\n",
    "#split between test and train set because we want to see the performance of our models\n",
    "def split_data(x, y):\n",
    "    if SEED is not None:\n",
    "        np.random.seed(SEED) \n",
    "    indices = np.arange(len(x)) #create an array of indices\n",
    "    np.random.shuffle(indices) #shuffle the indices\n",
    "\n",
    "    split_point = int(len(indices) * (1 - TEST_SIZE)) #compute the split point\n",
    "\n",
    "    train_x, train_y = np.array(x)[indices[:split_point]], np.array(y)[indices[:split_point]] #split the data for the train set\n",
    "    test_x, test_y = np.array(x)[indices[split_point:]], np.array(y)[indices[split_point:]] #split the data for the test set\n",
    "\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "# load images\n",
    "def load_image(infilename):\n",
    "    data = mpimg.imread(infilename) #load the image\n",
    "    return data\n",
    "\n",
    "# Extract 6-dimensional features consisting of average RGB color as well as variance\n",
    "def extract_features(img):\n",
    "    feat_m = np.mean(img, axis=(0,1)) #compute the mean of the image\n",
    "    feat_v = np.var(img, axis=(0,1)) #compute the variance of the image\n",
    "    feat = np.append(feat_m, feat_v) #append the mean and the variance\n",
    "    return feat\n",
    "\n",
    "# Compute features for each image patch\n",
    "def value_to_class(v):\n",
    "    df = np.sum(v) \n",
    "    if df > THRESHOLD:\n",
    "        return 1 #if the sum of the values is above the threshold, we return 1\n",
    "    else:\n",
    "        return 0 #if the sum of the values is below the threshold, we return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the road segmentation configuration class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadSegmentationConfig():\n",
    "    def __init__(self, divide_patches, normalize, augment, clahe):\n",
    "        self.divide_patches= divide_patches\n",
    "        self.norm=normalize\n",
    "        self.aug=augment\n",
    "        self.clahe=clahe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define preprocessing and postprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize images between 0 and 1\n",
    "def normalize_images(data):\n",
    "    for i, image in enumerate(data):\n",
    "        image = image / 255\n",
    "        data[i] = (image - image.mean(axis=(0, 1), dtype='float64')) / (image.std(axis=(0, 1), dtype='float64')) #normalize the image\n",
    "    return data\n",
    "\n",
    "# augment images\n",
    "def augment_images(data):\n",
    "    augmented_images = []\n",
    "\n",
    "    # Flip, rotate, blur and change brightness of the images\n",
    "    for image in data:\n",
    "        augmented_imgs = [\n",
    "            image.transpose(Image.FLIP_LEFT_RIGHT),\n",
    "            image.transpose(Image.ROTATE_90),\n",
    "            image.transpose(Image.ROTATE_180),\n",
    "            image.transpose(Image.ROTATE_270),\n",
    "            image.transpose(Image.FLIP_TOP_BOTTOM),\n",
    "            image.filter(ImageFilter.GaussianBlur(4)),\n",
    "        ]\n",
    "\n",
    "        color_shift = ImageEnhance.Color(image)\n",
    "        augmented_imgs.append(color_shift.enhance(0.5)) #increase brightness\n",
    "\n",
    "        for angle in range(10, 61, 10):\n",
    "            augmented_imgs.append(image.rotate(angle, resample=Image.BICUBIC))\n",
    "        augmented_images.extend(augmented_imgs)\n",
    "\n",
    "    return augmented_images\n",
    "\n",
    "# apply CLAHE to the dataset\n",
    "def apply_clahe_to_dataset(data, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
    "    enhanced_data = []\n",
    "\n",
    "    # Apply CLAHE to each image in the dataset\n",
    "    for image in data:\n",
    "        # Convert the image to grayscale if it's a color image\n",
    "        if len(image.shape) == 3:  # Check if the image is color\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray = image\n",
    "\n",
    "        # Create a CLAHE object\n",
    "        clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "\n",
    "        # Apply CLAHE to the grayscale image\n",
    "        clahe_image = clahe.apply(gray)\n",
    "\n",
    "        # If the original image is color, merge the enhanced channel with the original channels\n",
    "        if len(image.shape) == 3:\n",
    "            clahe_image = cv2.cvtColor(clahe_image, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        enhanced_data.append(clahe_image)\n",
    "\n",
    "    return np.array(enhanced_data)\n",
    "\n",
    "# crop images into patches\n",
    "def crop(image, width, height):\n",
    "    res = []\n",
    "    for i in range(0,image.shape[1],height):\n",
    "        for j in range(0,image.shape[0],width):\n",
    "            if len(image.shape) == 2: #if the image is grayscale\n",
    "                res.append(image[j:j + width, i : i + height]) #crop the image\n",
    "            else:\n",
    "                res.append(image[j:j + width, i : i + height, :]) #crop the image\n",
    "    return res\n",
    "\n",
    "\n",
    "# aplly preprocessing to the data\n",
    "def preprocess(data, gts, configuration:RoadSegmentationConfig, train: bool, ps):\n",
    "    data = images_to_np_array(data)\n",
    "\n",
    "    if train:\n",
    "        gts = images_to_np_array(gts) #transform the groundtruths into np.arrays\n",
    "    else:\n",
    "        gts = None\n",
    "\n",
    "    if configuration.norm:\n",
    "        data = normalize_images(data) #normalize the images\n",
    "\n",
    "    if not train:\n",
    "        return data, None\n",
    "\n",
    "    if configuration.aug:\n",
    "        data = PIL_Images_from_np_array(data) #transform the images into PIL images\n",
    "\n",
    "        if gts is not None:\n",
    "            gts = PIL_Images_from_np_array(gts) #transform the groundtruths into PIL images\n",
    "\n",
    "        augmented_images = augment_images(data) #augment the images\n",
    "\n",
    "        if gts is not None:\n",
    "            augmented_groundtruths = augment_images(gts) #augment the groundtruths\n",
    "            gts.extend(augmented_groundtruths) #add the augmented groundtruths to the groundtruths\n",
    "\n",
    "        data.extend(augmented_images) #add the augmented images to the images\n",
    "        data = images_to_np_array(data) #transform the images into np.arrays\n",
    "\n",
    "        if gts is not None:\n",
    "            gts = images_to_np_array(gts) #transform the groundtruths into np.arrays\n",
    "\n",
    "    if configuration.divide_patches and gts is not None: \n",
    "        patch_size = ps \n",
    "        data = [crop(image, patch_size,patch_size) for image in data] #crop the images\n",
    "        data = np.asarray([data[i][j] for i in range(len(data)) for j in range(len(data[i]))]) #transform the images into np.arrays\n",
    "        gts = [crop(image, patch_size, patch_size) for image in gts] #crop the groundtruths\n",
    "        gts = np.asarray([gts[i][j] for i in range(len(gts)) for j in range(len(gts[i]))]) #transform the groundtruths into np.arrays\n",
    "        data = images_to_np_array(data) #transform the images into np.arrays\n",
    "        gts = images_to_np_array(gts) #transform the groundtruths into np.arrays\n",
    "\n",
    "    if configuration.clahe:\n",
    "        data= apply_clahe_to_dataset(data) #apply CLAHE to the images\n",
    "        gts= apply_clahe_to_dataset(gts) #apply CLAHE to the groundtruths\n",
    "\n",
    "    return data, gts\n",
    "\n",
    "# load data\n",
    "def load_data(path_data, path_gts, train : bool, device,configuration:RoadSegmentationConfig, ps):\n",
    "    data = [Image.open(img) for img in path_data] #load the images\n",
    "    \n",
    "    gts = []\n",
    "    if path_gts is not None:\n",
    "        gts = [Image.open(gt) for gt in path_gts] #load the groundtruths\n",
    "\n",
    "    if train:\n",
    "        data, gts = preprocess(data, gts, configuration,True, ps) #apply preprocessing to the data and the groundtruths\n",
    "    else:\n",
    "        data, _ = preprocess(data, None, configuration, False, ps) #apply preprocessing to the data\n",
    "\n",
    "    return data, gts\n",
    "\n",
    "# apply postprocessing to the prediction\n",
    "def postprocess(y):\n",
    "    res = []\n",
    "    y = np.array(y)\n",
    "    sz = y[0][0].shape[0] #get the size of the image\n",
    "    y = y.reshape((-1,6,sz,sz)) #reshape the prediction\n",
    "    for images in y:\n",
    "        one = np.fliplr(images[1]) #flip the image\n",
    "        two = np.rot90(images[2], k=3) #rotate the image\n",
    "        three = np.rot90(images[3], k=2) #rotate the image\n",
    "        four = np.rot90(images[4],k=1) #rotate the image\n",
    "        five = np.flipud(images[5]) #flip the image\n",
    "\n",
    "        m = np.stack([images[0], one, two, three, four, five]) #stack the images\n",
    "        m = np.mean(m,axis=0) #compute the mean of the images\n",
    "        res.append(m) #append the mean to the result\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the possible preprocessing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divide_patches = True\n",
    "normalize = False\n",
    "augment = True\n",
    "training = True\n",
    "validation = False\n",
    "submmission = True\n",
    "clahe=True\n",
    "baseline = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadSegmentationDataset(Dataset):\n",
    "    def __init__(self, data_path, gt_path, configuration:RoadSegmentationConfig, train: bool, device = None, ps=PATCH_SIZE):\n",
    "        self.train = train\n",
    "        self.device = device\n",
    "        imgs, gts = load_data(data_path,gt_path,train,device,configuration, ps) #load the data and the groundtruths\n",
    "        divide = not configuration.norm #if we normalize the images, we don't divide by 255\n",
    "        if gts is not None: \n",
    "            self.gt = [mask_to_tensor(gt,device) for gt in gts] #transform the groundtruths into tensors\n",
    "        self.data = [image_into_tensor(img, device, divide = divide) for img in imgs] #transform the images into tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) #return the length of the data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            return self.data[index], self.gt[index] #return the data and the groundtruths\n",
    "        else:\n",
    "            return self.data[index] #return the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to create the image to insert in the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(prediction, dir):\n",
    "    img = Image.fromarray(prediction)\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB') #convert the image into RGB\n",
    "    img_path = os.path.join(dir, \"prediction.png\") #create the path of the image\n",
    "    img.save(img_path)  #save the image\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadSegmentationModel(nn.Module):\n",
    "    def __init__(self, device, lr = LR, th =THRESHOLD, max_iter = MAX_ITER, ps=PATCH_SIZE):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.pre_trained_network = get_deeplabplus()\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.pre_trained_network.to(self.device)\n",
    "        self.th = th\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.ps = ps\n",
    "    \n",
    "    #forward pass\n",
    "    def forward(self, data):\n",
    "        data_x = data[0].to(self.device) #data to the device\n",
    "        return self.pre_trained_network(data_x) #return the prediction\n",
    "\n",
    "    #train the model for one epoch\n",
    "    def train_epoch(self, loader, optimizer):\n",
    "        loss = []\n",
    "        self.pre_trained_network.train() #set the model in training mode\n",
    "        for batch in tqdm(loader): \n",
    "            pr = self.forward(batch) #predictions\n",
    "            y = batch[1].to(self.device) #labels\n",
    "            l = self.criterion(pr,y).to(self.device) #loss\n",
    "            optimizer.zero_grad() #set the gradients to zero\n",
    "            l.backward() #compute the gradients\n",
    "            optimizer.step() #update the weights\n",
    "            l = l.cpu().detach().numpy() #compute loss\n",
    "            loss.append(l)\n",
    "        return np.mean(loss)\n",
    "\n",
    "    #test the model for one epoch\n",
    "    def test_epoch(self, loader):\n",
    "        self.pre_trained_network.eval() #set the model in evaluation mode\n",
    "        loss = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(loader):\n",
    "                pr = self.forward(batch) #predictions\n",
    "                y = batch[1].to(self.device) #labels\n",
    "                l = self.criterion(pr,y).to(self.device) #loss\n",
    "                l = l.cpu().detach().numpy() #compute loss\n",
    "                loss.append(l)\n",
    "        return np.mean(loss)\n",
    "\n",
    "    #get the score of the model\n",
    "    def get_score(self,loader, do_postprocessing=True):\n",
    "        self.pre_trained_network.eval()\n",
    "        prs = []\n",
    "        ys = []\n",
    "        first_predictions = self.make_prediction(loader,do_postprocessing) #make a prediction\n",
    "        masks = loader.dataset.gt #get the groundtruths\n",
    "        #transform the prediction into patches\n",
    "        for pr, y in zip(first_predictions, masks):\n",
    "            labels, _ = transform_prediction_to_patch(pr,1, patch_size =self.ps, th=self.th) #transform the prediction into patches\n",
    "            prs.extend(labels) #extend the list of predictions\n",
    "            y = y[0].cpu().detach().numpy() #get the mask\n",
    "            real_labels, _ = transform_prediction_to_patch(y,1, patch_size =self.ps, th=self.th) #transform the mask into patches\n",
    "            ys.extend(real_labels) #extend the list of labels\n",
    "        return f1_score(ys,prs), accuracy_score(ys,prs)\n",
    "\n",
    "    #make a prediction\n",
    "    def make_prediction(self, loader, do_postprocessing):\n",
    "        pr = []\n",
    "        self.pre_trained_network.eval() #set the model in evaluation mode\n",
    "        sigmoid = torch.nn.Sigmoid() #apply sigmoid to the output\n",
    "        with torch.no_grad(): \n",
    "            for batch in tqdm(loader):\n",
    "                p = sigmoid(self.pre_trained_network(batch.to(self.device))).cpu().detach().numpy() #apply sigmoid to the output\n",
    "                pr.append(p[0][0]) #append the prediction\n",
    "        if do_postprocessing:\n",
    "            pr = postprocess(pr) #apply postprocessing\n",
    "        return pr\n",
    "\n",
    "    #train the model\n",
    "    def train(self, train_loader, test_loader, evaluate, evaluate_loader, do_postprocessing):\n",
    "        optimizer = torch.optim.Adam(self.pre_trained_network.parameters(), lr=self.lr) #define the optimizer\n",
    "        losses = []\n",
    "        test_losses = []\n",
    "        accuracies = []\n",
    "        f1s = []\n",
    "        best_loss = {'loss': float('inf'), 'epoch': 0}\n",
    "        i = 1\n",
    "        while True:\n",
    "            print(\"EPOCH \" + str(i))\n",
    "            l_train = self.train_epoch(train_loader, optimizer) #train the model\n",
    "            print(\"epoch trained, now testing\") \n",
    "            losses.append(l_train) #append the loss\n",
    "            l_test = self.test_epoch(test_loader) #test the model\n",
    "            test_losses.append(l_test) #append the loss\n",
    "            if evaluate:\n",
    "                print(\"Test Loss for epoch \" + str(i) + \" = \" + str(l_test))\n",
    "            else:\n",
    "                print(\"Train Loss for epoch \" + str(i) + \" = \" + str(l_test))\n",
    "            f1 = 0\n",
    "            acc = 0\n",
    "            if evaluate:\n",
    "                f1, acc = self.get_score(evaluate_loader, do_postprocessing) #get the score\n",
    "                print(\"LOSS = \" + str(l_test) + \" F1 = \" + str(f1) + \" ACCURACY = \" + str(acc))\n",
    "\n",
    "            f1s.append(f1)\n",
    "            accuracies.append(acc)\n",
    "\n",
    "            if l_test < best_loss['loss']: \n",
    "                best_loss['loss'] = l_test\n",
    "                best_loss['epoch'] = i\n",
    "\n",
    "            if i == self.max_iter - 1:\n",
    "                break\n",
    "\n",
    "            i += 1\n",
    "        \n",
    "        # save the results into a dictionary\n",
    "        results = {}\n",
    "        results['train_loss'] = losses\n",
    "        results['f1'] = f1s\n",
    "        results['accuracy'] = accuracies\n",
    "        results['test_loss'] = test_losses\n",
    "        return results\n",
    "\n",
    "    #create the submission file\n",
    "    def submit(self, test_loader):\n",
    "\n",
    "        prs = self.make_prediction(test_loader,False) #make a prediction\n",
    "        img_ids = range(1,len(prs)+1) #get the ids of the images\n",
    "\n",
    "        ret_ids = []\n",
    "        ret_labels = []\n",
    "\n",
    "        for pr, i in zip(prs,img_ids):\n",
    "            #create the image thanks to the prediction, of the 6 image of the test set\n",
    "            if i == 6:\n",
    "                pred = report(pr, self.th) # save the prediction to create the image for the report\n",
    "                plot_prediction(pred, SUBMISSION_DIR) #save the image\n",
    "\n",
    "            labels, ids = transform_prediction_to_patch(pr,i,th=self.th) #transform the prediction into patches\n",
    "            for label in labels:\n",
    "                ret_labels.append(label) #extend the list of predictions\n",
    "            for id in ids:\n",
    "                ret_ids.append(id) #extend the list of ids\n",
    "\n",
    "        pd.DataFrame({'id': ret_ids, 'prediction' : ret_labels}).to_csv(SUBMISSION_PATH,index=False) #create the submission file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the k indices for cross validation\n",
    "def build_k_indices(N, k_fold, seed = SEED):\n",
    "    num_row = N\n",
    "    interval = int(num_row / k_fold) #size of each fold\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row) #random permutation of the indices\n",
    "    k_indices = [indices[k * interval : (k + 1) * interval] for k in range(k_fold)] #split the indices into k_fold\n",
    "    return np.array(k_indices)\n",
    "\n",
    "# cross validation step\n",
    "def cross_validation_step(data, gts, k_indices, k, th, lr, ps):\n",
    "    train_data = np.delete(data, k_indices[k], axis = 0) #get the train data\n",
    "    train_gts = np.delete(gts, k_indices[k], axis = 0) #get the train labels\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    for id in k_indices[k]:\n",
    "        test_x.append(data[id]) #get the test data\n",
    "        test_y.append(gts[id]) #get the test labels\n",
    "    configuration=RoadSegmentationConfig (augment=True, normalize=True, divide_patches=True, clahe=True) #define the configuration\n",
    "\n",
    "    train_set = RoadSegmentationDataset(train_data,train_gts,configuration, True, DEVICE, ps) #create the train set\n",
    "    test_set = RoadSegmentationDataset(test_x,test_y,configuration,True,DEVICE, ps) #create the test set\n",
    "    evaluation_dataset = RoadSegmentationDataset(test_x,test_y,configuration,False,DEVICE, ps) #create the evaluation set\n",
    "\n",
    "    train_loader = DataLoader(train_set, BATCH_SIZE, shuffle=True) #create the train loader\n",
    "    test_loader = DataLoader(test_set,BATCH_SIZE, shuffle=False) #create the test loader\n",
    "    evaluation_loader = DataLoader(evaluation_dataset,1, shuffle=False) #create the evaluation loader\n",
    "\n",
    "    model = RoadSegmentationModel(DEVICE,th=th,lr=lr,max_iter=10) #create the model\n",
    "    results = model.train(train_loader, test_loader,True,evaluation_loader,False) #train the model\n",
    "    f1_score = results[\"f1\"] #get the f1 score\n",
    "    return f1_score\n",
    "\n",
    "# cross validation over the patch size\n",
    "def validation_over_patch_size(data, gts, k_indices, k, ps):\n",
    "    return cross_validation_step(data,gts,k_indices,k,THRESHOLD,LR, ps)\n",
    "\n",
    "# cross validation over the threshold\n",
    "def validation_over_threshold(data, gts, k_indices, k, threshold):\n",
    "    return cross_validation_step(data,gts,k_indices,k,threshold,LR, PATCH_SIZE)\n",
    "\n",
    "# cross validation over the learning rate\n",
    "def validation_over_learning_rate(data, gts, k_indices, k, lr):\n",
    "    return cross_validation_step(data,gts,k_indices,k,THRESHOLD,lr, PATCH_SIZE)\n",
    "\n",
    "# cross validation\n",
    "def cross_validation(data, gts, parameters, parameter_name, N, seed = SEED, k_fold = K_FOLD):\n",
    "    k_indices = build_k_indices(N,k_fold,seed) #build the k indices\n",
    "    best_performance = -1 \n",
    "    optimal_parameter = -1\n",
    "\n",
    "    for parameter in parameters:\n",
    "        print(\"Trying \" + str(parameter_name) + \" = \" + str(parameter))\n",
    "        avg_performance = 0 #average performance\n",
    "        performances = np.zeros(k_fold) #performances for each fold\n",
    "\n",
    "        for k in range(k_fold):\n",
    "            performance = 0\n",
    "            if parameter_name == \"threshold\":\n",
    "                performance = validation_over_threshold(data,gts,k_indices,k,parameter)[-1] #get the performance for the threshold\n",
    "            elif parameter_name == \"learning rate\":\n",
    "                performance = validation_over_learning_rate(data,gts,k_indices,k,parameter)[-1] #get the performance for the learning rate\n",
    "            elif parameter_name == \"patch size\":\n",
    "                performance = validation_over_patch_size(data,gts,k_indices,k,parameter)[-1] #get the performance for the patch size\n",
    "\n",
    "            avg_performance = performance + avg_performance \n",
    "            performances[k] = performance #save the performance for the fold\n",
    "\n",
    "        avg_performance = avg_performance / k_fold #compute the average performance\n",
    "\n",
    "        print(\"Cross-Validation for \" + parameter_name + \" = \" + str(parameter) + \" with f1_score = \" + str(avg_performance))\n",
    "        if best_performance == -1 or avg_performance > best_performance: #if the performance is better than the best performance\n",
    "            best_performance = avg_performance\n",
    "            optimal_parameter = parameter\n",
    "\n",
    "    print(\"Optimal Patameter for \" + parameter_name + \" = \" + str(optimal_parameter))\n",
    "    return optimal_parameter, best_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data for the logistic regression train\n",
    "def load_data_log_reg_train(path_data, path_gts):\n",
    "    files = os.listdir(path_data) #get the files\n",
    "    n=len(files) #number of files\n",
    "    imgs = [load_image(path_data + files[i]) for i in range(n)] #load the images\n",
    "    gt_dir = path_gts  # ground truth directory\n",
    "    print(\"Load \" + str(n) + \" images\") \n",
    "\n",
    "    gt_imgs = [load_image(gt_dir + files[i]) for i in range(n)] #load the ground truth images \n",
    "\n",
    "    img_patches = [crop(imgs[i], 16, 16) for i in range(n)] #crop the images into patches\n",
    "    gt_patches = [crop(gt_imgs[i], 16, 16) for i in range(n)] #crop the ground truth images into patches\n",
    "\n",
    "    # Linearize list of patches\n",
    "    img_patches = np.asarray([img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))])\n",
    "    gt_patches =  np.asarray([gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))])\n",
    "\n",
    "    X = np.asarray([ extract_features(img_patches[i]) for i in range(len(img_patches))]) #extract features\n",
    "    Y = np.asarray([value_to_class(np.mean(gt_patches[i])) for i in range(len(gt_patches))]) #get the labels\n",
    "    return X, Y\n",
    "\n",
    "# load data for the logistic regression test\n",
    "def load_data_log_reg_test(path_data):\n",
    "    files = os.listdir(path_data) #get the files\n",
    "    n=len(files)\n",
    "    imgs=[]\n",
    "    for j in range (n):\n",
    "        imgs.append(load_image(path_data + files[j] + \"/\" + files[j] + \".png\")) #load the images \n",
    "\n",
    "    print(\"Load \" + str(n) + \" images\")\n",
    "    img_patches = [crop(imgs[i], 16, 16) for i in range(n)] #crop the images into patches\n",
    "\n",
    "    # Linearize list of patches\n",
    "    img_patches = np.asarray([img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))])\n",
    "    X = np.asarray([ extract_features(img_patches[i]) for i in range(len(img_patches))]) #extract features\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "# logistic regression\n",
    "def log_regression (x, y, x_test):\n",
    "\n",
    "    logreg = linear_model.LogisticRegression(C=1e5, class_weight=\"balanced\") #define the model\n",
    "    logreg.fit(x, y) #train the model\n",
    "    # Predict on the training set to print\n",
    "    predict_test = logreg.predict(x_test)\n",
    "    # Predict on the train (to verify logreg.predict working)\n",
    "    predict=logreg.predict(x)\n",
    "\n",
    "    # Get non-zeros in prediction and grountruth arrays\n",
    "    predict_n = np.nonzero(predict)[0]\n",
    "    y_n = np.nonzero(y)[0]\n",
    "\n",
    "    TPR = len(list(set(y_n) & set(predict_n))) / float(len(predict)) #compute the true positive rate\n",
    "    accuracy = accuracy_score(y, predict) #compute the accuracy\n",
    "    f1 = f1_score(y, predict) #compute the f1 score\n",
    "    print('True positive rate = ' + str(TPR))\n",
    "    print('Accuracy score = ' + str(accuracy))\n",
    "    print('F1 score = ' + str(f1))\n",
    "\n",
    "    print (predict_test)\n",
    "    return predict_test\n",
    "\n",
    "# create the submission file for the logistic regression\n",
    "def log_reg_submit (predict_test):\n",
    "    img_ids = range(1,len(predict_test)+1) #get the ids of the images\n",
    "    ret_ids = []\n",
    "    ret_labels = []\n",
    "    predict_test=np.asarray(predict_test) #transform the prediction into an array\n",
    "    k=0\n",
    "    j=0\n",
    "    l=1\n",
    "    for pr, i in zip(predict_test,img_ids): #create the submission file\n",
    "        if (j>592):\n",
    "          j=0\n",
    "          k+=16\n",
    "        if (k>592):\n",
    "          j=0\n",
    "          k=0\n",
    "          l+=1\n",
    "\n",
    "        id=\"{:03d}_{}_{}\".format(l, k,j) #get the id of the image\n",
    "        j+=16\n",
    "        ret_labels.append(pr) \n",
    "        ret_ids.append(id)\n",
    "\n",
    "    pd.DataFrame({'id': ret_ids, 'prediction' : ret_labels}).to_csv(SUBMISSION_PATH,index=False) #create the submission file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define all the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = 4\n",
    "\n",
    "if EXPERIMENT == 1:\n",
    "    divide_patches = True\n",
    "    normalize = False\n",
    "    augment = False\n",
    "    training = True\n",
    "    submmission = True\n",
    "    clahe=True\n",
    "elif EXPERIMENT == 2:\n",
    "    divide_patches = True\n",
    "    normalize = True\n",
    "    augment = False\n",
    "    training = True\n",
    "    submmission = True\n",
    "    clahe=True\n",
    "elif EXPERIMENT == 3:\n",
    "    divide_patches = True\n",
    "    normalize = False\n",
    "    augment = True\n",
    "    training = True\n",
    "    submmission = True\n",
    "    clahe=True\n",
    "\n",
    "#EXPERIMENT 4 is the best one\n",
    "elif EXPERIMENT == 4:\n",
    "    divide_patches = True\n",
    "    normalize = True\n",
    "    augment = True\n",
    "    training = True\n",
    "    submmission = True\n",
    "    clahe=False\n",
    "    baseline = False\n",
    "    # PATCH_SIZE = 400\n",
    "    # LR = 0.001\n",
    "    # THRESHOLD = 0.17\n",
    "\n",
    "elif EXPERIMENT == 5:\n",
    "    divide_patches = True\n",
    "    normalize = True\n",
    "    augment = True\n",
    "    training = True\n",
    "    submmission = True\n",
    "    clahe=True\n",
    "    baseline = False\n",
    "\n",
    "#EXPERIMENT 6 is the baseline\n",
    "if EXPERIMENT == 6:\n",
    "    divide_patches = False\n",
    "    normalize = False\n",
    "    augment = False\n",
    "    training = False\n",
    "    submmission = False\n",
    "    clahe = False\n",
    "    baseline = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the parameter used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(THRESHOLD)\n",
    "print(LR)\n",
    "print(PATCH_SIZE)\n",
    "print(MAX_ITER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and call the run function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the run function\n",
    "def run():\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    configuration = RoadSegmentationConfig(augment=augment, normalize=normalize, divide_patches=divide_patches, clahe=clahe) #define the configuration\n",
    "\n",
    "    data = sorted(glob(DATA_PATH + \"/training/images/*.png\")) #get the images\n",
    "    gts = sorted(glob(DATA_PATH + \"/training/groundtruth/*.png\")) #get the ground truth images\n",
    "\n",
    "    if baseline :\n",
    "        submission_images = sorted(glob(DATA_PATH + \"/test_set_images/*/*\"), #get the submission images\n",
    "        key = lambda x: int(x.split('/')[-2].split('_')[-1])) \n",
    "        x, y = load_data_log_reg_train(DATA_PATH + \"/training/images/\", DATA_PATH + \"/training/groundtruth/\") #load the data for the logistic regression\n",
    "        x_test = load_data_log_reg_test(DATA_PATH + \"/test_set_images/\") #load the data for the logistic regression\n",
    "        predict_regression=log_regression (x, y, x_test) #logistic regression\n",
    "        log_reg_submit (predict_regression) #create the submission file\n",
    "        return\n",
    "\n",
    "    optimal_th = THRESHOLD\n",
    "    optimal_lr = LR\n",
    "    optimal_ps = PATCH_SIZE\n",
    "\n",
    "    # cross validation\n",
    "    if validation:\n",
    "        print(\"Starting Cross Validation over Foreground Threshold\")\n",
    "        optimal_th,_ = cross_validation(data,gts,THRESHOLD_VALIDATION_VECTOR,\"threshold\",len(data))\n",
    "        print(\"Starting Cross Validation over Learning Rate\")\n",
    "        optimal_lr,_ = cross_validation(data,gts,LEARNING_RATE_VALIDATION_VECTOR,\"learning rate\",len(data))\n",
    "        print(\"Starting Cross Validation over Patch size \")\n",
    "        optimal_ps,_ = cross_validation(data,gts,PATCH_SIZE_VALIDATION_VECTOR,\"patch size\",len(data))\n",
    "\n",
    "    train_data, train_labels, test_data, test_labels = split_data(data,gts) #split the data between train and test set\n",
    "\n",
    "    if submmission:\n",
    "        # run training on full dataset\n",
    "        train_data = data\n",
    "        train_labels = gts\n",
    "\n",
    "    train_set = RoadSegmentationDataset(train_data,train_labels,configuration, True, DEVICE, optimal_ps) #create the train set\n",
    "    test_set = RoadSegmentationDataset(test_data,test_labels,configuration,True,DEVICE, optimal_ps) #create the test set\n",
    "    evaluation_dataset = RoadSegmentationDataset(test_data,test_labels,configuration,False,DEVICE, optimal_ps) #create the evaluation set\n",
    "\n",
    "    train_loader = DataLoader(train_set, BATCH_SIZE, shuffle=True) #create the train loader\n",
    "    test_loader = DataLoader(test_set,BATCH_SIZE, shuffle=False)  #create the test loader\n",
    "    evaluation_loader = DataLoader(evaluation_dataset,1, shuffle=False) #create the evaluation loader\n",
    "\n",
    "    model = RoadSegmentationModel(DEVICE, th= optimal_th, lr = optimal_lr, max_iter = MAX_ITER, ps= optimal_ps) #create the model\n",
    "\n",
    "    if training:\n",
    "        if submmission:\n",
    "            results = model.train(train_loader, test_loader,False,None,augment) #train the model\n",
    "            f1 = None\n",
    "            acc = None\n",
    "        else: \n",
    "            results = model.train(train_loader, test_loader,True,evaluation_loader,False) #train the model\n",
    "            f1 = results['f1'] #get the f1 score\n",
    "            acc = results['accuracy'] #get the accuracy\n",
    "        loss = results['train_loss'] #get the train loss\n",
    "        test_loss = results['test_loss'] #get the test loss\n",
    "        print(\"TRAINING LOSS = \" + str(loss[len(loss)-1]))\n",
    "        print(\"TEST LOSS = \" + str(test_loss[len(test_loss)-1]))\n",
    "    if submmission:\n",
    "        submission_images = sorted(glob(DATA_PATH + \"/test_set_images/*/*\"), \n",
    "            key = lambda x: int(x.split('/')[-2].split('_')[-1])) #get the submission images \n",
    "\n",
    "        test_set = RoadSegmentationDataset(submission_images, None, configuration, False, DEVICE) #create the test set\n",
    "        test_loader = DataLoader(test_set,1) #create the test loader\n",
    "\n",
    "        model.submit(test_loader) #create the submission file\n",
    "\n",
    "    return f1, acc\n",
    "\n",
    "#call the run function and save the results (f1 score and accuracy)\n",
    "f1,acc= run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the f1 score and the accuracy over the epochs, for the evaluation set, do not run this if submission is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plot the f1 score and the accuracy\n",
    "def plot_f1(f1,acc):\n",
    "    plt.plot(f1, label = 'f1')\n",
    "    plt.plot(acc, label = 'accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.title('F1 & accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_f1(f1,acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
